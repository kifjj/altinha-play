{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNaAcP9Bd5f30t5hpdkTE8G","gpuType":"T4","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13828471,"sourceType":"datasetVersion","datasetId":8806920},{"sourceId":139474,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":118113,"modelId":141350},{"sourceId":143398,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":121493,"modelId":144637},{"sourceId":657326,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496889,"modelId":512253},{"sourceId":663636,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":502186,"modelId":517348},{"sourceId":656865,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496512,"modelId":511911}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Altinha Ball Tracking and Hit Detection\n\nThis notebook performs ball tracking and hit detection on altinha (Brazilian footvolley) videos using YOLO object detection.\n\n<a href=\"https://colab.research.google.com/github/kifjj/altinha-play/blob/main/alta_infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"markdown","source":"## Install Dependencies\n\nInstall required packages with specific versions for compatibility.\n","metadata":{}},{"cell_type":"code","source":"!pip install \"numpy<2.0\" \"scipy<1.14\" supervision ultralytics \"opencv-python-headless<4.12\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Libraries\n","metadata":{}},{"cell_type":"code","source":"import json\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom typing import Dict, List, Optional, Sequence, Tuple\nfrom ultralytics import YOLO\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configuration\n\nSet up paths and hyperparameters for ball detection and hit counting.\n","metadata":{}},{"cell_type":"code","source":"# Video and model paths\nVIDEO_PATH = '/kaggle/input/alta-videos/altinha-beach-green-mq-13s.mp4'\nMODEL_PATH = '/kaggle/input/yolo-ft-2511/pytorch/default/1/altinha_best.pt'\nOUTPUT_PATH = '/kaggle/working/altinha-beach-green-mq-BEST_ONLY.mp4'\n\nPOSE_MODEL_PATH = '/kaggle/input/yolo11-pose/pytorch/default/1/yolo11l-pose.pt'\n\n# Debug output directory for hit frames\nDEBUG_FRAMES_DIR = '/kaggle/working/debug_frames'\n\n# Detection parameters\nCONFIDENCE_THRESHOLD = 0.05  # Minimum confidence for initial detection\nMIN_CONFIDENCE = 0.08  # Minimum confidence to keep a detection\nIOU_NMS = 0.5  # NMS IoU threshold\n\n# Hit detection parameters\nMIN_VERTICAL_AMPLITUDE = 3  # Minimum pixels for a valid hit (vertical movement)\nMIN_FRAMES_BETWEEN_HITS = 8  # Minimum frames between consecutive hits\nGAP_RESET_FRAMES = 30  # Frames without detection before resetting trajectory (1 sec at 30fps)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initialize Models and Annotators\n","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Copy pose model to writable directory to avoid read-only file system error\npose_model_writable = '/kaggle/working/yolo11l-pose.pt'\nif not os.path.exists(pose_model_writable):\n    shutil.copy(POSE_MODEL_PATH, pose_model_writable)\n\n# Load YOLO models\nmodel = YOLO(MODEL_PATH)\nmodel_pose = YOLO(pose_model_writable)  # Load from writable location\n\n# Setup annotators\nbox_annotator = sv.BoxAnnotator(\n    thickness=2,\n    color=sv.Color.from_hex(\"#00FF00\")\n)\n\nlabel_annotator = sv.LabelAnnotator(\n    text_scale=0.5,\n    text_thickness=2,\n    text_position=sv.Position.TOP_CENTER,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions\n","metadata":{}},{"cell_type":"code","source":"from typing import Dict, List, Optional, Sequence, Tuple\n\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nBallPosition = Tuple[int, float, float, np.ndarray]\nHitDetections = List[Dict[str, object]]\n\n\ndef filter_best_ball_detection(detections: sv.Detections, min_confidence: float) -> sv.Detections:\n    \"\"\"\n    Filter detections to keep only the best one (highest confidence).\n    \n    Args:\n        detections: sv.Detections object\n        min_confidence: Minimum confidence threshold\n        \n    Returns:\n        Filtered sv.Detections object (empty if no valid detection)\n    \"\"\"\n    if len(detections) == 0:\n        return detections\n    \n    best_idx = int(np.argmax(detections.confidence))\n    best_conf = float(detections.confidence[best_idx])\n    \n    if best_conf >= min_confidence:\n        return detections[best_idx:best_idx+1]\n    \n    return detections[0:0]  # Return empty detections\n\n\ndef update_ball_tracking_state(\n    detections: sv.Detections,\n    n_frame: int,\n    last_ball_positions: List[BallPosition],\n    last_ball_detection_n_frame: Optional[int],\n    gap_reset_frames: int,\n) -> Tuple[List[BallPosition], Optional[int]]:\n    \"\"\"\n    Update ball tracking state with new detection.\n    \n    Args:\n        detections: sv.Detections object\n        n_frame: Current frame number\n        last_ball_positions: List of (frame_idx, x_center, y_center, bbox) tuples\n        last_ball_detection_n_frame: Last frame with detection\n        gap_reset_frames: Max gap before resetting trajectory\n        \n    Returns:\n        tuple: (updated_last_positions, updated_last_detection_frame)\n    \"\"\"\n    if len(detections) == 0:\n        return last_ball_positions, last_ball_detection_n_frame\n    \n    # Check for gap in detections\n    if last_ball_detection_n_frame is not None:\n        gap = n_frame - last_ball_detection_n_frame\n        if gap > gap_reset_frames:\n            last_ball_positions = []\n    \n    # Get ball center and bbox\n    bbox = detections.xyxy[0]\n    x1, y1, x2, y2 = bbox.tolist()\n    x_center = 0.5 * (x1 + x2)\n    y_center = 0.5 * (y1 + y2)\n    \n    # Update position history (keep last 3)\n    last_ball_positions = last_ball_positions.copy()\n    last_ball_positions.append((n_frame, x_center, y_center, bbox))\n    if len(last_ball_positions) > 3:\n        last_ball_positions.pop(0)\n    \n    return last_ball_positions, n_frame\n\n\ndef detect_hit(last_positions: List[BallPosition]) -> Tuple[bool, Optional[int], Optional[float], Optional[float], Optional[np.ndarray]]:\n    \"\"\"\n    Detect if a hit occurred based on ball trajectory.\n    A hit is detected when the ball reaches a local maximum in y-coordinate (bottom of screen).\n    \n    Args:\n        last_positions: List of (frame_idx, x_center, y_center, bbox) tuples (last 3 positions)\n        \n    Returns:\n        tuple: (is_hit, frame_idx, y_center, vertical_span, bbox) or (False, None, None, None, None)\n    \"\"\"\n    if len(last_positions) != 3:\n        return False, None, None, None, None\n    \n    (f0, x0, y0, bbox0), (f1, x1c, y1c, bbox1c), (f2, x2c, y2c, bbox2c) = last_positions\n    \n    # Check if middle point is a local maximum (ball at lowest point)\n    going_down_then_up = (y0 < y1c) and (y2c < y1c)\n    vertical_span = y1c - min(y0, y2c)\n    \n    if going_down_then_up and vertical_span >= MIN_VERTICAL_AMPLITUDE:\n        return True, f1, y1c, vertical_span, bbox1c\n    \n    return False, None, None, None, None\n\n\ndef get_pose_keypoints(frame: np.ndarray, model_pose: YOLO) -> List[np.ndarray]:\n    \"\"\"\n    Run YOLO-pose model to detect player keypoints in the frame.\n    \n    Args:\n        frame: Video frame to analyze\n        model_pose: YOLO pose model\n        \n    Returns:\n        List of poses, each containing keypoints data\n    \"\"\"\n    results = model_pose(frame, verbose=False, conf=0.3)[0]\n    \n    if results.keypoints is None or len(results.keypoints.data) == 0:\n        return []\n    \n    # Extract keypoints data\n    # keypoints shape: (num_persons, num_keypoints, 3) where 3 = (x, y, confidence)\n    poses = []\n    for person_keypoints in results.keypoints.data:\n        poses.append(person_keypoints.cpu().numpy())\n    \n    return poses\n\n\ndef find_closest_player(ball_center: Tuple[float, float], poses: Sequence[np.ndarray]) -> Tuple[Optional[np.ndarray], int]:\n    \"\"\"\n    Find the player closest to the ball.\n    \n    Args:\n        ball_center: Tuple (x, y) of ball center\n        poses: List of pose keypoints arrays\n        \n    Returns:\n        Tuple (player_pose, player_id) or (None, -1) if no players detected\n    \"\"\"\n    if not poses:\n        return None, -1\n    \n    ball_x, ball_y = ball_center\n    min_distance = float('inf')\n    closest_player_idx = -1\n    \n    print(f\"  [FIND_CLOSEST_PLAYER] Found {len(poses)} poses\")\n\n    for i, pose in enumerate(poses):\n        # Calculate player center from valid keypoints\n        valid_keypoints = pose[pose[:, 2] > 0.3]  # Filter by confidence > 0.3\n        if len(valid_keypoints) == 0:\n            continue\n        \n        player_x = np.mean(valid_keypoints[:, 0])\n        player_y = np.mean(valid_keypoints[:, 1])\n        \n        # Calculate distance to ball\n        distance = np.sqrt((ball_x - player_x)**2 + (ball_y - player_y)**2)\n        \n        if distance < min_distance:\n            min_distance = distance\n            closest_player_idx = i\n    \n    if closest_player_idx == -1:\n        return None, -1\n    \n    return poses[closest_player_idx], closest_player_idx\n\n\ndef classify_hit_type(ball_bbox: np.ndarray, player_pose: Optional[np.ndarray]) -> str:\n    \"\"\"\n    Classify hit type (Head/Foot/Unknown) based on ball position and player keypoints.\n    \n    Args:\n        ball_bbox: Ball bounding box in xyxy format\n        player_pose: Player keypoint array (17, 3) with (x, y, confidence)\n        \n    Returns:\n        String: 'Head', 'Foot', or 'Unknown'\n    \"\"\"\n    if player_pose is None:\n        print(\"  [CLASSIFY] No player pose detected -> Unknown\")\n        return 'Unknown'\n    \n    # Calculate ball center\n    ball_x = (ball_bbox[0] + ball_bbox[2]) / 2\n    ball_y = (ball_bbox[1] + ball_bbox[3]) / 2\n\n    print(f\"\\n  [CLASSIFY] Ball position: x={ball_x:.1f}, y={ball_y:.1f}\")\n    \n    # COCO keypoint indices:\n    # Head: 0=nose, 3=left_ear, 4=right_ear\n    # Feet: 15=left_ankle, 16=right_ankle\n    head_indices = [0, 3, 4]\n    foot_indices = [15, 16]\n    head_names = ['nose', 'left_ear', 'right_ear']\n    foot_names = ['left_ankle', 'right_ankle']\n    \n    # Calculate distance to head keypoints\n    head_distances = []\n    print(f\"  [CLASSIFY] Head keypoints:\")\n    for idx, name in zip(head_indices, head_names):\n        if idx < len(player_pose) and player_pose[idx, 2] > 0.3:  # Check confidence\n            kp_x, kp_y, kp_conf = player_pose[idx, 0], player_pose[idx, 1], player_pose[idx, 2]\n            dist = np.sqrt((ball_x - kp_x)**2 + (ball_y - kp_y)**2)\n            head_distances.append(dist)\n            print(f\"    - {name:12s}: pos=({kp_x:.1f}, {kp_y:.1f}), conf={kp_conf:.2f}, dist={dist:.1f}\")\n        else:\n            conf_str = f\"{player_pose[idx, 2]:.2f}\" if idx < len(player_pose) else \"N/A\"\n            print(f\"    - {name:12s}: SKIPPED (conf={conf_str})\")\n    \n    # Calculate distance to foot keypoints\n    foot_distances = []\n    print(f\"  [CLASSIFY] Foot keypoints:\")\n    for idx, name in zip(foot_indices, foot_names):\n        if idx < len(player_pose) and player_pose[idx, 2] > 0.3:  # Check confidence\n            kp_x, kp_y, kp_conf = player_pose[idx, 0], player_pose[idx, 1], player_pose[idx, 2]\n            dist = np.sqrt((ball_x - kp_x)**2 + (ball_y - kp_y)**2)\n            foot_distances.append(dist)\n            print(f\"    - {name:12s}: pos=({kp_x:.1f}, {kp_y:.1f}), conf={kp_conf:.2f}, dist={dist:.1f}\")\n        else:\n            conf_str = f\"{player_pose[idx, 2]:.2f}\" if idx < len(player_pose) else \"N/A\"\n            print(f\"    - {name:12s}: SKIPPED (conf={conf_str})\")\n    \n    # Determine hit type based on minimum distances\n    min_head_dist = min(head_distances) if head_distances else float('inf')\n    min_foot_dist = min(foot_distances) if foot_distances else float('inf')\n    \n    print(f\"  [CLASSIFY] Min distances: head={min_head_dist:.1f}, foot={min_foot_dist:.1f}\")\n    \n    # If both are unavailable\n    if min_head_dist == float('inf') and min_foot_dist == float('inf'):\n        print(f\"  [CLASSIFY] No valid keypoints detected -> Unknown\")\n        return 'Unknown'\n    \n    # Classification with threshold (prefer the closer one with a margin)\n    distance_threshold = 80  # pixels - adjust based on video resolution\n    print(f\"  [CLASSIFY] Distance threshold: {distance_threshold} pixels\")\n    \n    # Decision logic\n    if min_head_dist < distance_threshold and min_head_dist < min_foot_dist:\n        print(f\"  [CLASSIFY] DECISION: Head (head_dist {min_head_dist:.1f} < threshold {distance_threshold} AND head_dist < foot_dist {min_foot_dist:.1f})\")\n        return 'Head'\n    elif min_foot_dist < distance_threshold:\n        print(f\"  [CLASSIFY] DECISION: Foot (foot_dist {min_foot_dist:.1f} < threshold {distance_threshold})\")\n        return 'Foot'\n    else:\n        print(f\"  [CLASSIFY] DECISION: Unknown (both distances exceed threshold: head={min_head_dist:.1f}, foot={min_foot_dist:.1f})\")\n        return 'Unknown'\n\n\n\n\ndef draw_debug_keypoints(frame: np.ndarray, player_pose: Optional[np.ndarray]) -> np.ndarray:\n    \"\"\"\n    Draw small red boxes at nose, left ankle, and right ankle for debugging.\n    \n    Args:\n        frame: Video frame to annotate\n        player_pose: Player keypoint array (17, 3) with (x, y, confidence)\n        \n    Returns:\n        Annotated frame\n    \"\"\"\n    if player_pose is None:\n        return frame\n    \n    # COCO keypoint indices: 0=nose, 3=left_ear, 4=right_ear, 15=left_ankle, 16=right_ankle\n    keypoint_indices = [0, 3, 4, 15, 16]\n    keypoint_names = ['nose', 'left_ear', 'right_ear', 'left_ankle', 'right_ankle']\n    colors = [(0, 0, 255), (0, 0, 255), (0, 0, 255), (0, 0, 255), (0, 0, 255)]  # Red for all\n    \n    for idx, name, color in zip(keypoint_indices, keypoint_names, colors):\n        if idx < len(player_pose) and player_pose[idx, 2] > 0.3:  # Check confidence\n            kp_x, kp_y = int(player_pose[idx, 0]), int(player_pose[idx, 1])\n            \n            # Draw small red box (5x5 pixels)\n            box_size = 5\n            cv2.rectangle(\n                frame,\n                (kp_x - box_size, kp_y - box_size),\n                (kp_x + box_size, kp_y + box_size),\n                color,\n                thickness=2\n            )\n            \n            # Draw label next to the box\n            cv2.putText(\n                frame,\n                name,\n                (kp_x + box_size + 2, kp_y),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.4,\n                color,\n                1,\n                cv2.LINE_AA\n            )\n    \n    return frame\n\ndef check_and_record_hit(\n    last_ball_positions: List[BallPosition],\n    hit_detections: HitDetections,\n    fps: float,\n    min_frames_between_hits: int,\n    frame: np.ndarray,\n    model_pose: YOLO,\n    frame_buffer: Optional[Sequence[Tuple[int, np.ndarray]]] = None,\n    debug_dir: Optional[str] = None,\n) -> HitDetections:\n    \"\"\"\n    Check for hit and record it if valid, with hit type classification.\n    \n    Args:\n        last_positions: List of (frame_idx, x_center, y_center, bbox) tuples\n        hit_detections: List of hit metadata dicts\n        fps: Video frames per second\n        min_frames_between_hits: Minimum frames between consecutive hits\n        frame: Current video frame (fallback if frame_buffer lookup fails)\n        model_pose: YOLO pose model\n        frame_buffer: Optional list of (frame_idx, frame_image) tuples for debug output and frame retrieval\n        debug_dir: Optional directory to save debug frames\n        \n    Returns:\n        Updated hit_detections list\n    \"\"\"\n    is_hit, hit_n_frame, hit_y, span, hit_bbox = detect_hit(last_ball_positions)\n    \n    if is_hit:\n        # Check minimum gap between hits\n        last_hit_frame = hit_detections[-1]['frame'] if hit_detections else None\n        if not hit_detections or (hit_n_frame - last_hit_frame) >= min_frames_between_hits:\n            # Retrieve the correct frame from frame_buffer matching hit_n_frame\n            hit_frame = frame  # fallback to current frame\n            if frame_buffer is not None:\n                for frame_idx, frame_image in frame_buffer:\n                    if frame_idx == hit_n_frame:\n                        hit_frame = frame_image\n                        break\n            \n            # Analyze pose to classify hit type using the correct frame\n            poses = get_pose_keypoints(hit_frame, model_pose)\n            \n            ball_center = ((hit_bbox[0] + hit_bbox[2]) / 2, (hit_bbox[1] + hit_bbox[3]) / 2)\n            \n            player_pose, player_id = find_closest_player(ball_center, poses)\n            \n            hit_type = classify_hit_type(hit_bbox, player_pose)\n            \n            hit_detections = hit_detections.copy()\n            hit_detections.append({\n                'frame': hit_n_frame,\n                'type': hit_type,\n                'player_id': player_id,\n                'player_pose': player_pose  # Store pose for visualization\n            })\n            t_sec = hit_n_frame / fps\n            hit_number = len(hit_detections)\n            print(f\"HIT #{hit_number} at frame {hit_n_frame} (t={t_sec:.2f}s), Type: {hit_type}, Player: {player_id}, y={hit_y:.1f}, span={span:.1f}\")\n            \n            # Save debug frames if frame buffer is available\n            if frame_buffer is not None and debug_dir is not None:\n                save_debug_frames(frame_buffer, hit_n_frame, hit_number, debug_dir)\n            \n            return hit_detections\n    \n    return hit_detections\n\n\ndef annotate_frame(\n    frame: np.ndarray,\n    detections: sv.Detections,\n    box_annotator: sv.BoxAnnotator,\n    label_annotator: sv.LabelAnnotator,\n    hit_detections: HitDetections,\n    n_frame: int,\n) -> np.ndarray:\n    \"\"\"\n    Annotate frame with ball detection, hit counter, and debug keypoints.\n    \n    Args:\n        frame: Video frame to annotate\n        detections: sv.Detections object\n        box_annotator: Supervision box annotator\n        label_annotator: Supervision label annotator\n        hit_detections: List of hit metadata dicts\n        n_frame: Current frame number\n        \n    Returns:\n        Annotated frame\n    \"\"\"\n    annotated_frame = frame.copy()\n    \n    # Draw ball detection box and label\n    if len(detections) > 0:\n        conf = float(detections.confidence[0])\n        labels = [f\"Ball {conf:.2f}\"]\n        annotated_frame = box_annotator.annotate(\n            scene=annotated_frame,\n            detections=detections,\n        )\n        annotated_frame = label_annotator.annotate(\n            scene=annotated_frame,\n            detections=detections,\n            labels=labels,\n        )\n    \n    # Draw debug keypoints for the most recent hit (show for 10 frames after hit)\n    if hit_detections:\n        last_hit = hit_detections[-1]\n        frames_since_hit = n_frame - last_hit['frame']\n        if 0 <= frames_since_hit <= 10 and 'player_pose' in last_hit:\n            annotated_frame = draw_debug_keypoints(annotated_frame, last_hit['player_pose'])\n    \n    # Draw hit counter HUD\n    annotated_frame = draw_hit_counter(annotated_frame, hit_detections)\n    \n    return annotated_frame\n\n\ndef draw_hit_counter(frame: np.ndarray, hit_detections: Sequence[Dict[str, object]]) -> np.ndarray:\n    \"\"\"\n    Draw a hit counter HUD on the frame with breakdown by hit type.\n    \n    Args:\n        frame: Video frame to annotate\n        hit_detections: List of hit metadata dicts\n        \n    Returns:\n        Annotated frame\n    \"\"\"\n    # Calculate hit counts by type\n    total_hits = len(hit_detections)\n    head_hits = sum(1 for h in hit_detections if h['type'] == 'Head')\n    foot_hits = sum(1 for h in hit_detections if h['type'] == 'Foot')\n    unknown_hits = sum(1 for h in hit_detections if h['type'] == 'Unknown')\n    \n    hit_text = f\"Hits: {total_hits} | Head: {head_hits} | Foot: {foot_hits} | Unknown: {unknown_hits}\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.8\n    thickness = 2\n    \n    # Measure text size\n    (text_width, text_height), baseline = cv2.getTextSize(\n        hit_text, font, font_scale, thickness\n    )\n    \n    # Box position and padding\n    pad_x, pad_y = 10, 10\n    x1, y1 = 10, 10\n    x2 = x1 + text_width + 2 * pad_x\n    y2 = y1 + text_height + 2 * pad_y\n    \n    # Draw filled rectangle\n    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), thickness=-1)\n    \n    # Draw text\n    text_x = x1 + pad_x\n    text_y = y1 + pad_y + text_height\n    cv2.putText(\n        frame, hit_text, (text_x, text_y),\n        font, font_scale, (0, 255, 0), thickness, cv2.LINE_AA\n    )\n    \n    return frame\n\n\ndef save_debug_frames(\n    frame_buffer: Sequence[Tuple[int, np.ndarray]],\n    hit_frame: int,\n    hit_number: int,\n    debug_dir: str,\n) -> None:\n    \"\"\"\n    Save the 3 frames around a detected hit for debugging.\n    \n    Args:\n        frame_buffer: List of (frame_idx, frame_image) tuples\n        hit_frame: Frame number where the hit occurred\n        hit_number: Sequential hit number (1-indexed)\n        debug_dir: Directory to save debug frames\n    \"\"\"\n    import os\n    \n    # Create debug directory if it doesn't exist\n    os.makedirs(debug_dir, exist_ok=True)\n    \n    # Find frames to save: hit_frame-1, hit_frame, hit_frame+1\n    frames_to_save = [hit_frame - 1, hit_frame, hit_frame + 1]\n    \n    for frame_idx, frame_image in frame_buffer:\n        if frame_idx in frames_to_save:\n            filename = f\"hit-{hit_number}-frame-{frame_idx}.png\"\n            filepath = os.path.join(debug_dir, filename)\n            cv2.imwrite(filepath, frame_image)\n            print(f\"  ðŸ’¾ Saved debug frame: {filename}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def debug_print_detections(detections: sv.Detections, n_frame: int) -> None:\n    if len(detections) > 0:\n        conf = float(detections.confidence[0])\n        x1, y1, x2, y2 = detections.xyxy[0].tolist()\n\n        print(\n            f\"FRAME {n_frame:4d}: best conf={conf:.3f}, \"\n            f\"bbox=({x1:.1f},{y1:.1f},{x2:.1f},{y2:.1f})\"\n        )\n    else:\n        print(f\"FRAME {n_frame:4d}: no detection\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process Video\n\nRun ball detection and hit counting on the video.\n","metadata":{}},{"cell_type":"code","source":"# Get video info\nvideo_info = sv.VideoInfo.from_video_path(VIDEO_PATH)\nfps = video_info.fps\nframes_generator = sv.get_video_frames_generator(VIDEO_PATH)\n\nprint(f\"Processing {VIDEO_PATH}\")\nprint(f\"FPS: {fps}, Resolution: {video_info.width}x{video_info.height}\\n\")\n\n# Initialize tracking state\nlast_ball_positions = []  # Track last 3 (frame_idx, x_center, y_center, bbox) positions\nhit_detections = []  # List of hit metadata dicts: {'frame': int, 'type': str, 'player_id': int}\nlast_ball_detection_n_frame = None  # Last frame with a detection\nframe_buffer = []  # Buffer to keep last 3 frames for debug output\nprevious_frame = None\n\n# Detection statistics\nprocessed_frames = 0\nframes_with_ball_detection = 0\n\n# Process video\nwith sv.VideoSink(target_path=OUTPUT_PATH, video_info=video_info) as sink:\n    for n_frame, frame in enumerate(frames_generator, start=1):\n\n        processed_frames += 1\n\n        if previous_frame is None:\n            previouse_frame = frame\n\n        # Add current frame to buffer (keep last 3 frames)\n        frame_buffer.append((n_frame, frame.copy()))\n        if len(frame_buffer) > 3:\n            frame_buffer.pop(0)\n\n        # Run YOLO detection\n        results = model(\n            frame,\n            verbose=False,\n            conf=CONFIDENCE_THRESHOLD,\n            iou=IOU_NMS,\n        )[0]\n\n        ball_detections = sv.Detections.from_ultralytics(results)\n\n        # Filter to keep only best detection\n        ball_detections = filter_best_ball_detection(ball_detections, MIN_CONFIDENCE)\n\n        debug_print_detections(ball_detections, n_frame)\n\n        if len(ball_detections) > 0:\n            frames_with_ball_detection += 1\n\n        # Update tracking state\n        last_ball_positions, last_ball_detection_n_frame = update_ball_tracking_state(\n            ball_detections,\n            n_frame,\n            last_ball_positions,\n            last_ball_detection_n_frame,\n            GAP_RESET_FRAMES\n        )\n\n        # Check for hit and record it\n        if len(ball_detections) > 0:\n            hit_detections = check_and_record_hit(\n                last_ball_positions,\n                hit_detections,\n                fps,\n                MIN_FRAMES_BETWEEN_HITS,\n                previous_frame,\n                model_pose,\n                frame_buffer=frame_buffer,\n                debug_dir=DEBUG_FRAMES_DIR\n            )\n\n        # Annotate frame with detections and hit counter\n        annotated_frame = annotate_frame(\n            frame, ball_detections, box_annotator, label_annotator, hit_detections, n_frame\n        )\n\n        # Write frame to output video\n        sink.write_frame(annotated_frame)\n\n        previous_frame = frame\n\ndetection_percentage = frames_with_ball_detection / processed_frames if processed_frames > 0 else 0.0\n\nprint(f\"Done! Video saved to {OUTPUT_PATH}\")\nprint(f\"ðŸ“Š Total hits detected: {len(hit_detections)}\")\nprint(f\"âš½ Ball detection percentage: {detection_percentage:.3f}\")\nprint(f\"ðŸ› Debug frames saved to: {DEBUG_FRAMES_DIR}\")\n\nvideo_filename = os.path.basename(VIDEO_PATH)\njson_output_path = os.path.join(\n    os.path.dirname(OUTPUT_PATH) or \".\",\n    os.path.splitext(video_filename)[0] + \".json\"\n)\n\nsummary_data = {\n    \"video_name\": video_filename,\n    \"ball_detection_percentage\": detection_percentage\n}\n\nwith open(json_output_path, \"w\") as json_file:\n    json.dump(summary_data, json_file, indent=4)\n\nprint(f\"Detection summary saved to {json_output_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results Summary\n\nDisplay detailed results of hit detection.\n","metadata":{}},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"HIT DETECTION RESULTS\")\nprint(\"=\" * 60)\n\n# Calculate hit statistics\ntotal_hits = len(hit_detections)\nhead_hits = sum(1 for h in hit_detections if h['type'] == 'Head')\nfoot_hits = sum(1 for h in hit_detections if h['type'] == 'Foot')\nunknown_hits = sum(1 for h in hit_detections if h['type'] == 'Unknown')\n\nprint(f\"Total hits/passes detected: {total_hits}\")\nprint(f\"  - Head hits: {head_hits} ({100*head_hits/total_hits:.1f}%)\" if total_hits > 0 else \"  - Head hits: 0\")\nprint(f\"  - Foot hits: {foot_hits} ({100*foot_hits/total_hits:.1f}%)\" if total_hits > 0 else \"  - Foot hits: 0\")\nprint(f\"  - Unknown: {unknown_hits} ({100*unknown_hits/total_hits:.1f}%)\" if total_hits > 0 else \"  - Unknown: 0\")\n\nprint(f\"\\nDetailed timestamps:\")\nprint(\"-\" * 60)\n\nfor i, hit_data in enumerate(hit_detections, start=1):\n    frame_idx = hit_data['frame']\n    hit_type = hit_data['type']\n    player_id = hit_data['player_id']\n    timestamp = frame_idx / fps\n    minutes = int(timestamp // 60)\n    seconds = timestamp % 60\n    print(f\"  Hit #{i:2d} | Frame {frame_idx:4d} | {minutes:02d}:{seconds:05.2f} | {hit_type:7s} | Player {player_id}\")\n\nprint(\"=\" * 60)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}